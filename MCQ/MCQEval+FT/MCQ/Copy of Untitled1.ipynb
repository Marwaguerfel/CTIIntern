{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47469,"status":"ok","timestamp":1727171822011,"user":{"displayName":"marwa guerfel","userId":"05259411296939797730"},"user_tz":-60},"id":"WiZCh1djecpt","outputId":"36a5c9e2-91a7-4bcb-f694-c10c0916cb73"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# 1. Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":94027,"status":"ok","timestamp":1727171916031,"user":{"displayName":"marwa guerfel","userId":"05259411296939797730"},"user_tz":-60},"id":"ejYlyHQult1g"},"outputs":[],"source":["%%capture\n","# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n","!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n","\n","# We have to check which Torch version for Xformers (2.3 -\u003e 0.0.27)\n","from torch import __version__; from packaging.version import Version as V\n","xformers = \"xformers==0.0.27\" if V(__version__) \u003c V(\"2.4.0\") else \"xformers\"\n","!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":71018,"status":"ok","timestamp":1727171987038,"user":{"displayName":"marwa guerfel","userId":"05259411296939797730"},"user_tz":-60},"id":"wDv19hQxfU7U","outputId":"0cb3aaa6-c0c0-4b04-935c-25a83d7256d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors\u003e=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers\u003c0.20,\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.23.2-\u003etransformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.23.2-\u003etransformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2.2.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2024.8.30)\n","Collecting jsonlines\n","\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(\u003cpip._vendor.urllib3.connection.HTTPSConnection object at 0x7c424d66f160\u003e, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/f8/62/d9ba6323b9202dd2fe166beab8a86d29465c41a0288cbe229fac60c1ab8d/jsonlines-4.0.0-py3-none-any.whl.metadata\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(\u003cpip._vendor.urllib3.connection.HTTPSConnection object at 0x7c424d66f3d0\u003e, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/f8/62/d9ba6323b9202dd2fe166beab8a86d29465c41a0288cbe229fac60c1ab8d/jsonlines-4.0.0-py3-none-any.whl.metadata\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(\u003cpip._vendor.urllib3.connection.HTTPSConnection object at 0x7c424d66f670\u003e, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/f8/62/d9ba6323b9202dd2fe166beab8a86d29465c41a0288cbe229fac60c1ab8d/jsonlines-4.0.0-py3-none-any.whl.metadata\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(\u003cpip._vendor.urllib3.connection.HTTPSConnection object at 0x7c424d66f820\u003e, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/f8/62/d9ba6323b9202dd2fe166beab8a86d29465c41a0288cbe229fac60c1ab8d/jsonlines-4.0.0-py3-none-any.whl.metadata\u001b[0m\u001b[33m\n","\u001b[0m  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: attrs\u003e=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (24.2.0)\n","Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n","Installing collected packages: jsonlines\n","Successfully installed jsonlines-4.0.0\n"]}],"source":["!pip install transformers\n","!pip install jsonlines"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1727171987039,"user":{"displayName":"marwa guerfel","userId":"05259411296939797730"},"user_tz":-60},"id":"cROi4rp4iSsK"},"outputs":[],"source":["\n","# 2. Import necessary libraries\n","import jsonlines\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import os\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1727171987039,"user":{"displayName":"marwa guerfel","userId":"05259411296939797730"},"user_tz":-60},"id":"cY-e4pZyjhlx"},"outputs":[],"source":["save_directory = '/content/drive/MyDrive/modelMyCTIBD'\n","if False:\n","    from unsloth import FastLanguageModel\n","    model, tokenizer = FastLanguageModel.from_pretrained(\n","        model_name =save_directory, # YOUR MODEL YOU USED FOR TRAINING\n","        max_seq_length = max_seq_length,\n","        dtype = dtype,\n","        load_in_4bit = load_in_4bit,\n","    )\n","    FastLanguageModel.for_inference(model) # Enable native 2x faster inference"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1460,"status":"ok","timestamp":1727171988486,"user":{"displayName":"marwa guerfel","userId":"05259411296939797730"},"user_tz":-60},"id":"NETALnm_lMjT"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(save_directory)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":64325,"status":"ok","timestamp":1727172052805,"user":{"displayName":"marwa guerfel","userId":"05259411296939797730"},"user_tz":-60},"id":"HxYiokEalgGx","outputId":"59f04dfd-9d1b-4aca-9d55-19ee17d231bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","==((====))==  Unsloth 2024.9.post2: Fast Llama patching. Transformers = 4.44.2.\n","   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n","O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ef2611198bbc4ed58e5f10225f461333","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/5.70G [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c35bcc1eb2c44bdb14960305461b5ce","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/230 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Unsloth 2024.9.post2 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"]},{"data":{"text/plain":["PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): LlamaForCausalLM(\n","      (model): LlamaModel(\n","        (embed_tokens): Embedding(128256, 4096)\n","        (layers): ModuleList(\n","          (0-31): 32 x LlamaDecoderLayer(\n","            (self_attn): LlamaAttention(\n","              (q_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=4096, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=4096, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (k_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=4096, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=1024, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (v_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=4096, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=1024, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (o_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=4096, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=4096, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (rotary_emb): LlamaExtendedRotaryEmbedding()\n","            )\n","            (mlp): LlamaMLP(\n","              (gate_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=4096, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=14336, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (up_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=4096, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=14336, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (down_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Identity()\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=14336, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=4096, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (act_fn): SiLU()\n","            )\n","            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n","            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n","          )\n","        )\n","        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n","        (rotary_emb): LlamaRotaryEmbedding()\n","      )\n","      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n","    )\n","  )\n",")"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["from unsloth import FastLanguageModel\n","\n","# Load the model with 4-bit quantization using `unsloth`\n","model, _ = FastLanguageModel.from_pretrained( # Assuming the model is the first element of the tuple\n","    model_name=save_directory,\n","    dtype=torch.float16,\n","    load_in_4bit=True\n",")\n","FastLanguageModel.for_inference(model)  # Enable native 2x faster inference"]},{"cell_type":"markdown","metadata":{"id":"9jldS8YSn-iJ"},"source":["datasetiiiy"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":749,"status":"ok","timestamp":1727172057489,"user":{"displayName":"marwa guerfel","userId":"05259411296939797730"},"user_tz":-60},"id":"uthyfnff0Dwj"},"outputs":[],"source":["import pandas as pd\n","import json\n","\n","# Load the TSV file\n","df = pd.read_csv('/content/output_questions_20(2).tsv', sep='\\t')\n","df.columns = df.columns.str.strip()\n","\n","# Create a list to hold the JSON objects\n","data = []\n","\n","# Process each row in the DataFrame\n","for _, row in df.iterrows():\n","    entry = {\n","        'question': row['Question'],\n","        'options': {\n","            'A': row['Option A'],\n","            'B': row['Option B'],\n","            'C': row['Option C'],\n","            'D': row['Option D']\n","        },\n","        'correct_answer': row['Correct Answer']\n","    }\n","    data.append(entry)\n","\n","# Save to JSONL file\n","with open('/content/dataJson.jsonl', 'w') as f:\n","    for item in data:\n","        f.write(json.dumps(item) + '\\n')\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":653,"status":"ok","timestamp":1727172060782,"user":{"displayName":"marwa guerfel","userId":"05259411296939797730"},"user_tz":-60},"id":"UvJCfD7C0bU9","outputId":"3dd3cda6-691d-40ec-f11b-4d69e558364f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset has been cleaned and saved to /content/cleaned_dataJson.jsonl\n"]}],"source":["import json\n","\n","input_file = \"/content/dataJson.jsonl\"\n","output_file = \"/content/cleaned_dataJson.jsonl\"\n","\n","# Function to sanitize options (convert all option values to strings)\n","def sanitize_options(options):\n","    for key, value in options.items():\n","        options[key] = str(value)  # Ensure all options are strings\n","    return options\n","\n","# Open the original dataset and the output file\n","with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n","    for line in infile:\n","        try:\n","            # Parse each line as JSON\n","            example = json.loads(line)\n","\n","            # Sanitize the 'options' field\n","            if 'options' in example:\n","                example['options'] = sanitize_options(example['options'])\n","\n","            # Write the sanitized example back to the new file\n","            outfile.write(json.dumps(example) + \"\\n\")\n","        except json.JSONDecodeError as e:\n","            print(f\"Error decoding JSON: {e}\")\n","            continue  # Skip any lines with JSON errors\n","\n","print(\"Dataset has been cleaned and saved to\", output_file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"scMGWxt30jZs"},"outputs":[{"name":"stdout","output_type":"stream","text":["Evaluation Accuracy: 56.51%\n"]}],"source":["\n","\n","# Load the dataset from a JSONL file\n","dataset_path = '/content/cleaned_dataJson.jsonl'  # Change this path\n","mcq_dataset = []\n","\n","with jsonlines.open(dataset_path) as reader:\n","    for obj in reader:\n","        mcq_dataset.append(obj)\n","\n","# Alpaca-style prompt template\n","alpaca_prompt = \"Question: {}\\nOptions:\\n{}\\nAnswer:\"\n","\n","# Function to prepare the prompt\n","def prepare_prompt(question, options):\n","    # Ensure options is a dictionary and format correctly\n","    if isinstance(options, dict):\n","        options_text = \"\\n\".join([f\"{key}: {value}\" for key, value in options.items()])\n","    else:\n","        options_text = options  # Handle the case where options are already formatted as a string\n","    return alpaca_prompt.format(question, options_text)\n","\n","# Evaluation function using generation\n","def evaluate_mcq_with_generation(question, options, correct_answer, tokenizer, model):\n","    # Prepare the prompt\n","    prompt = prepare_prompt(question, options)\n","\n","    # Tokenize the prompt\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Generate the output\n","    with torch.no_grad():\n","        outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n","\n","    # Decode the output and get the predicted answer\n","    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    # Extract the predicted answer (adapting based on the model's output format)\n","    if \"Answer:\" in generated_text:\n","        predicted_answer = generated_text.split(\"Answer:\")[-1].strip()  # Extracting the answer after \"Answer:\"\n","    else:\n","        predicted_answer = generated_text.strip()  # In case the model doesn't output \"Answer:\"\n","\n","    # Compare predicted answer with the correct answer\n","    return predicted_answer.lower() == correct_answer.lower()  # Case-insensitive comparison\n","\n","# Evaluate the entire dataset\n","correct_predictions = 0\n","total_questions = len(mcq_dataset)\n","\n","for mcq in mcq_dataset:\n","    question = mcq.get(\"question\", \"\")\n","    options = mcq.get(\"options\", \"\")\n","    correct_answer = mcq.get(\"correct_answer\", \"\")\n","\n","    # Check if correct_answer is valid before evaluating\n","    if isinstance(correct_answer, str):\n","        if evaluate_mcq_with_generation(question, options, correct_answer, tokenizer, model):\n","            correct_predictions += 1\n","\n","# Calculate and print accuracy\n","accuracy = correct_predictions / total_questions\n","print(f\"Evaluation Accuracy: {accuracy * 100:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"BkoBItY3tmsS"},"source":["Exemple on other dataset\n","ayoubData"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qN6tVqWqtmFo"},"outputs":[],"source":["import pandas as pd\n","import json\n","\n","# Load the TSV file\n","df = pd.read_csv('/content/DatasetGen.csv')\n","df.columns = df.columns.str.strip()\n","\n","# Create a list to hold the JSON objects\n","data = []\n","\n","# Process each row in the DataFrame\n","for _, row in df.iterrows():\n","    entry = {\n","        'question': row['Question'],\n","        'options': {\n","            'A': row['Option A'],\n","            'B': row['Option B'],\n","            'C': row['Option C'],\n","            'D': row['Option D']\n","        },\n","        'correct_answer': row['GT']\n","    }\n","    data.append(entry)\n","\n","# Save to JSONL file\n","with open('/content/dataJson.jsonl', 'w') as f:\n","    for item in data:\n","        f.write(json.dumps(item) + '\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aeUGofdit4WF"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset has been cleaned and saved to /content/cleaned_dataJson.jsonl\n"]}],"source":["import json\n","\n","input_file = \"/content/dataJson.jsonl\"\n","output_file = \"/content/cleaned_dataJson.jsonl\"\n","\n","# Function to sanitize options (convert all option values to strings)\n","def sanitize_options(options):\n","    for key, value in options.items():\n","        options[key] = str(value)  # Ensure all options are strings\n","    return options\n","\n","# Open the original dataset and the output file\n","with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n","    for line in infile:\n","        try:\n","            # Parse each line as JSON\n","            example = json.loads(line)\n","\n","            # Sanitize the 'options' field\n","            if 'options' in example:\n","                example['options'] = sanitize_options(example['options'])\n","\n","            # Write the sanitized example back to the new file\n","            outfile.write(json.dumps(example) + \"\\n\")\n","        except json.JSONDecodeError as e:\n","            print(f\"Error decoding JSON: {e}\")\n","            continue  # Skip any lines with JSON errors\n","\n","print(\"Dataset has been cleaned and saved to\", output_file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dCAQRIBFt7V_"},"outputs":[],"source":["\n","\n","# Load the dataset from a JSONL file\n","dataset_path = '/content/cleaned_dataJson.jsonl'  # Change this path\n","mcq_dataset = []\n","\n","with jsonlines.open(dataset_path) as reader:\n","    for obj in reader:\n","        mcq_dataset.append(obj)\n","\n","# Alpaca-style prompt template\n","alpaca_prompt = \"Question: {}\\nOptions:\\n{}\\nAnswer:\"\n","\n","# Function to prepare the prompt\n","def prepare_prompt(question, options):\n","    # Ensure options is a dictionary and format correctly\n","    if isinstance(options, dict):\n","        options_text = \"\\n\".join([f\"{key}: {value}\" for key, value in options.items()])\n","    else:\n","        options_text = options  # Handle the case where options are already formatted as a string\n","    return alpaca_prompt.format(question, options_text)\n","\n","# Evaluation function using generation\n","def evaluate_mcq_with_generation(question, options, correct_answer, tokenizer, model):\n","    # Prepare the prompt\n","    prompt = prepare_prompt(question, options)\n","\n","    # Tokenize the prompt\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Generate the output\n","    with torch.no_grad():\n","        outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n","\n","    # Decode the output and get the predicted answer\n","    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    # Extract the predicted answer (adapting based on the model's output format)\n","    if \"Answer:\" in generated_text:\n","        predicted_answer = generated_text.split(\"Answer:\")[-1].strip()  # Extracting the answer after \"Answer:\"\n","    else:\n","        predicted_answer = generated_text.strip()  # In case the model doesn't output \"Answer:\"\n","\n","    # Compare predicted answer with the correct answer\n","    return predicted_answer.lower() == correct_answer.lower()  # Case-insensitive comparison\n","\n","# Evaluate the entire dataset\n","correct_predictions = 0\n","total_questions = len(mcq_dataset)\n","\n","for mcq in mcq_dataset:\n","    question = mcq.get(\"question\", \"\")\n","    options = mcq.get(\"options\", \"\")\n","    correct_answer = mcq.get(\"correct_answer\", \"\")\n","\n","    # Check if correct_answer is valid before evaluating\n","    if isinstance(correct_answer, str):\n","        if evaluate_mcq_with_generation(question, options, correct_answer, tokenizer, model):\n","            correct_predictions += 1\n","\n","# Calculate and print accuracy\n","accuracy = correct_predictions / total_questions\n","print(f\"Evaluation Accuracy: {accuracy * 100:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"n3gCJtsPnv4h"},"source":["dataset meryam"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jPaw2gj-msB-"},"outputs":[],"source":["import pandas as pd\n","import json\n","\n","# Load the TSV file\n","df = pd.read_csv('/content/myDB (3).tsv', sep='\\t')\n","df.columns = df.columns.str.strip()\n","\n","# Create a list to hold the JSON objects\n","data = []\n","\n","# Process each row in the DataFrame\n","for _, row in df.iterrows():\n","    entry = {\n","        'question': row['question'],\n","        'options': {\n","            'A': row['Option_A'],\n","            'B': row['Option_B'],\n","            'C': row['Option_C'],\n","            'D': row['Option_D']\n","        },\n","        'correct_answer': row['Correct_Answer']\n","    }\n","    data.append(entry)\n","\n","# Save to JSONL file\n","with open('/content/dataJson.jsonl', 'w') as f:\n","    for item in data:\n","        f.write(json.dumps(item) + '\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6IVHHRXmr4o"},"outputs":[],"source":["import json\n","\n","input_file = \"/content/dataJson.jsonl\"\n","output_file = \"/content/cleaned_dataJson.jsonl\"\n","\n","# Function to sanitize options (convert all option values to strings)\n","def sanitize_options(options):\n","    for key, value in options.items():\n","        options[key] = str(value)  # Ensure all options are strings\n","    return options\n","\n","# Open the original dataset and the output file\n","with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n","    for line in infile:\n","        try:\n","            # Parse each line as JSON\n","            example = json.loads(line)\n","\n","            # Sanitize the 'options' field\n","            if 'options' in example:\n","                example['options'] = sanitize_options(example['options'])\n","\n","            # Write the sanitized example back to the new file\n","            outfile.write(json.dumps(example) + \"\\n\")\n","        except json.JSONDecodeError as e:\n","            print(f\"Error decoding JSON: {e}\")\n","            continue  # Skip any lines with JSON errors\n","\n","print(\"Dataset has been cleaned and saved to\", output_file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiiOZpa8mroO"},"outputs":[],"source":["\n","\n","# Load the dataset from a JSONL file\n","dataset_path = '/content/cleaned_dataJson.jsonl'  # Change this path\n","mcq_dataset = []\n","\n","with jsonlines.open(dataset_path) as reader:\n","    for obj in reader:\n","        mcq_dataset.append(obj)\n","\n","# Alpaca-style prompt template\n","alpaca_prompt = \"Question: {}\\nOptions:\\n{}\\nAnswer:\"\n","\n","# Function to prepare the prompt\n","def prepare_prompt(question, options):\n","    # Ensure options is a dictionary and format correctly\n","    if isinstance(options, dict):\n","        options_text = \"\\n\".join([f\"{key}: {value}\" for key, value in options.items()])\n","    else:\n","        options_text = options  # Handle the case where options are already formatted as a string\n","    return alpaca_prompt.format(question, options_text)\n","\n","# Evaluation function using generation\n","def evaluate_mcq_with_generation(question, options, correct_answer, tokenizer, model):\n","    # Prepare the prompt\n","    prompt = prepare_prompt(question, options)\n","\n","    # Tokenize the prompt\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Generate the output\n","    with torch.no_grad():\n","        outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n","\n","    # Decode the output and get the predicted answer\n","    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    # Extract the predicted answer (adapting based on the model's output format)\n","    if \"Answer:\" in generated_text:\n","        predicted_answer = generated_text.split(\"Answer:\")[-1].strip()  # Extracting the answer after \"Answer:\"\n","    else:\n","        predicted_answer = generated_text.strip()  # In case the model doesn't output \"Answer:\"\n","\n","    # Compare predicted answer with the correct answer\n","    return predicted_answer.lower() == correct_answer.lower()  # Case-insensitive comparison\n","\n","# Evaluate the entire dataset\n","correct_predictions = 0\n","total_questions = len(mcq_dataset)\n","\n","for mcq in mcq_dataset:\n","    question = mcq.get(\"question\", \"\")\n","    options = mcq.get(\"options\", \"\")\n","    correct_answer = mcq.get(\"correct_answer\", \"\")\n","\n","    # Check if correct_answer is valid before evaluating\n","    if isinstance(correct_answer, str):\n","        if evaluate_mcq_with_generation(question, options, correct_answer, tokenizer, model):\n","            correct_predictions += 1\n","\n","# Calculate and print accuracy\n","accuracy = correct_predictions / total_questions\n","print(f\"Evaluation Accuracy: {accuracy * 100:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"icRAvhq-np2c"},"source":["dataset demodel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9G-ri4-OnOqq"},"outputs":[],"source":["import pandas as pd\n","import json\n","\n","# Load the TSV file\n","df = pd.read_csv('/content/cti-mcq.tsv',sep='\\t')\n","df.columns = df.columns.str.strip()\n","\n","# Create a list to hold the JSON objects\n","data = []\n","\n","# Process each row in the DataFrame\n","for _, row in df.iterrows():\n","    entry = {\n","        'question': row['Question'],\n","        'options': {\n","            'A': row['Option A'],\n","            'B': row['Option B'],\n","            'C': row['Option C'],\n","            'D': row['Option D']\n","        },\n","        'correct_answer': row['GT']\n","    }\n","    data.append(entry)\n","\n","# Save to JSONL file\n","with open('/content/dataJson.jsonl', 'w') as f:\n","    for item in data:\n","        f.write(json.dumps(item) + '\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Jo223XbnOgD"},"outputs":[],"source":["import json\n","\n","input_file = \"/content/dataJson.jsonl\"\n","output_file = \"/content/cleaned_dataJson.jsonl\"\n","\n","# Function to sanitize options (convert all option values to strings)\n","def sanitize_options(options):\n","    for key, value in options.items():\n","        options[key] = str(value)  # Ensure all options are strings\n","    return options\n","\n","# Open the original dataset and the output file\n","with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n","    for line in infile:\n","        try:\n","            # Parse each line as JSON\n","            example = json.loads(line)\n","\n","            # Sanitize the 'options' field\n","            if 'options' in example:\n","                example['options'] = sanitize_options(example['options'])\n","\n","            # Write the sanitized example back to the new file\n","            outfile.write(json.dumps(example) + \"\\n\")\n","        except json.JSONDecodeError as e:\n","            print(f\"Error decoding JSON: {e}\")\n","            continue  # Skip any lines with JSON errors\n","\n","print(\"Dataset has been cleaned and saved to\", output_file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w0rCk7M3nONw"},"outputs":[],"source":["\n","\n","# Load the dataset from a JSONL file\n","dataset_path = '/content/cleaned_dataJson.jsonl'  # Change this path\n","mcq_dataset = []\n","\n","with jsonlines.open(dataset_path) as reader:\n","    for obj in reader:\n","        mcq_dataset.append(obj)\n","\n","# Alpaca-style prompt template\n","alpaca_prompt = \"Question: {}\\nOptions:\\n{}\\nAnswer:\"\n","\n","# Function to prepare the prompt\n","def prepare_prompt(question, options):\n","    # Ensure options is a dictionary and format correctly\n","    if isinstance(options, dict):\n","        options_text = \"\\n\".join([f\"{key}: {value}\" for key, value in options.items()])\n","    else:\n","        options_text = options  # Handle the case where options are already formatted as a string\n","    return alpaca_prompt.format(question, options_text)\n","\n","# Evaluation function using generation\n","def evaluate_mcq_with_generation(question, options, correct_answer, tokenizer, model):\n","    # Prepare the prompt\n","    prompt = prepare_prompt(question, options)\n","\n","    # Tokenize the prompt\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Generate the output\n","    with torch.no_grad():\n","        outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n","\n","    # Decode the output and get the predicted answer\n","    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    # Extract the predicted answer (adapting based on the model's output format)\n","    if \"Answer:\" in generated_text:\n","        predicted_answer = generated_text.split(\"Answer:\")[-1].strip()  # Extracting the answer after \"Answer:\"\n","    else:\n","        predicted_answer = generated_text.strip()  # In case the model doesn't output \"Answer:\"\n","\n","    # Compare predicted answer with the correct answer\n","    return predicted_answer.lower() == correct_answer.lower()  # Case-insensitive comparison\n","\n","# Evaluate the entire dataset\n","correct_predictions = 0\n","total_questions = len(mcq_dataset)\n","\n","for mcq in mcq_dataset:\n","    question = mcq.get(\"question\", \"\")\n","    options = mcq.get(\"options\", \"\")\n","    correct_answer = mcq.get(\"correct_answer\", \"\")\n","\n","    # Check if correct_answer is valid before evaluating\n","    if isinstance(correct_answer, str):\n","        if evaluate_mcq_with_generation(question, options, correct_answer, tokenizer, model):\n","            correct_predictions += 1\n","\n","# Calculate and print accuracy\n","accuracy = correct_predictions / total_questions\n","print(f\"Evaluation Accuracy: {accuracy * 100:.2f}%\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO211THvMBKbzFgcusuXtGM","gpuType":"T4","machine_shape":"hm","name":"","provenance":[{"file_id":"1ojE42k7zYf2YtsME1zeR9UlaF4GOOm1X","timestamp":1726769190358}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"01ffe1928ecc404a86f61c3b48649c1d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04571890999d41738207f57702ceeb49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b41053011314431cbb07a1e3bf75b438","placeholder":"â€‹","style":"IPY_MODEL_1731443c246b4ef8a4e3c3205d427ade","value":"â€‡230/230â€‡[00:00\u0026lt;00:00,â€‡17.8kB/s]"}},"0ec6812a76a240eab5a05247e0ba387c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1731443c246b4ef8a4e3c3205d427ade":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2484522105954b17b745479cc9fddaae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a7a1b1c7eea4c08a6fafeb6b501e0d2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f1764ae82e54424b50034532018eee5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40955a0e325447908b66453e9f69e549":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a3e8d547ad84427b1996ae3d106064b","max":230,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8415a5536d824118b113ed0c7a6d97a3","value":230}},"423a459b5515415184dac6276a4c601b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e33841d94044f35b94b9f75cf7afbae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6397c58b48e4b568036c6202feceb35","placeholder":"â€‹","style":"IPY_MODEL_2484522105954b17b745479cc9fddaae","value":"generation_config.json:â€‡100%"}},"50890cda65fb46a194970e6daab35503":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a3e8d547ad84427b1996ae3d106064b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fb82a76f2454eb5ad29b0c1b350fdb2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6cc81b11cfa94f3a8963f6969593b2fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8415a5536d824118b113ed0c7a6d97a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8ca700dbca7644a6a8134e0d259d1df9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a7a1b1c7eea4c08a6fafeb6b501e0d2","placeholder":"â€‹","style":"IPY_MODEL_423a459b5515415184dac6276a4c601b","value":"â€‡5.70G/5.70Gâ€‡[00:20\u0026lt;00:00,â€‡345MB/s]"}},"9c35bcc1eb2c44bdb14960305461b5ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4e33841d94044f35b94b9f75cf7afbae","IPY_MODEL_40955a0e325447908b66453e9f69e549","IPY_MODEL_04571890999d41738207f57702ceeb49"],"layout":"IPY_MODEL_6cc81b11cfa94f3a8963f6969593b2fd"}},"a6397c58b48e4b568036c6202feceb35":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b41053011314431cbb07a1e3bf75b438":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef2611198bbc4ed58e5f10225f461333":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fe61694d128e4e5d918ac67b25d555c8","IPY_MODEL_f8c9845db00f4e9dbd726f5b4c090684","IPY_MODEL_8ca700dbca7644a6a8134e0d259d1df9"],"layout":"IPY_MODEL_01ffe1928ecc404a86f61c3b48649c1d"}},"f8c9845db00f4e9dbd726f5b4c090684":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_50890cda65fb46a194970e6daab35503","max":5702746390,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3f1764ae82e54424b50034532018eee5","value":5702746390}},"fe61694d128e4e5d918ac67b25d555c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ec6812a76a240eab5a05247e0ba387c","placeholder":"â€‹","style":"IPY_MODEL_5fb82a76f2454eb5ad29b0c1b350fdb2","value":"model.safetensors:â€‡100%"}}}}},"nbformat":4,"nbformat_minor":0}